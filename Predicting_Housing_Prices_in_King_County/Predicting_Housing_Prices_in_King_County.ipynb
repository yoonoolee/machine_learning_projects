{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzAYMOSs584w"
      },
      "source": [
        "# Predicting Housing Prices in King County \n",
        "#### Team Members: Avery Lee, Pranav Natarajan, Malikah Nathani, Zach Zlepper\n",
        "#### Date: 6/2/2022\n",
        "###### Models: Linear Regression, Elastic Net, SVM Regression, AdaBoost Regression, Random Forest Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYrbZseu55GO"
      },
      "source": [
        "## Import libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fw7CKuN86t_y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEMb4q-_6veW"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "The dataset contains information about housing prices in the King County area. Our goal is to use this dataset to create models that predict the housing price given a property's features, and compare to see which models have better performance. The dataset is from Kaggle: https://www.kaggle.com/datasets/harlfoxem/housesalesprediction?resource=download\n",
        "\n",
        "We drop the date and id values because they are just identifiers or are too niche of a predictor. The zipcode and waterfront values are categorical so they are OneHotEncoded. Numeric features are normalized using StandardScaler. We split the train and test set by the default values, 75%/25%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iTssQRQl-3wJ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/sample_data/kc_house_data.csv')\n",
        "data = data.drop([\"date\"], axis=1)       # do not need the date of purchase. Too niche a predictor\n",
        "data = data.drop([\"id\"], axis=1)         # do not need the id. perfect predictor, makes no sense for out of sample cases\n",
        "\n",
        "## NEED TO CHANGE zipcode and waterfront to categorical variable by definition \n",
        "data.zipcode = data.zipcode.astype('category')\n",
        "data.waterfront = data.waterfront.astype('category')\n",
        "\n",
        "## Performing Train-Test Split\n",
        "## default split is 0.75-0.25\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(['price'], axis=1), \n",
        "                                                    data.price,\n",
        "                                                    random_state=42)\n",
        "# PIPELINE \n",
        "num_pipeline = Pipeline([\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "num_features = data.drop([\"waterfront\",\"zipcode\", \"price\"], axis=1)\n",
        "num_attribs = list(num_features)\n",
        "cat_attribs = [\"waterfront\",\"zipcode\"]\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs)])\n",
        "\n",
        "X_train_scaled = full_pipeline.fit_transform(X_train)\n",
        "X_test_scaled = full_pipeline.fit_transform(X_test)\n",
        "X_train_scaled.columns = X_train.columns.copy()\n",
        "X_test_scaled.columns = X_test.columns.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDZ_rXt8qLlh"
      },
      "source": [
        "Below is the dataset before preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "YZ-ncR6bPyaM",
        "outputId": "c1c73c55-ffd3-4d9b-b5b5-a0d705d69764"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a4739de1-8ddb-492f-91bb-2caafdc16250\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5945</th>\n",
              "      <td>4</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1810</td>\n",
              "      <td>9240</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1810</td>\n",
              "      <td>0</td>\n",
              "      <td>1961</td>\n",
              "      <td>0</td>\n",
              "      <td>98055</td>\n",
              "      <td>47.4362</td>\n",
              "      <td>-122.187</td>\n",
              "      <td>1660</td>\n",
              "      <td>9240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8423</th>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1600</td>\n",
              "      <td>2788</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1600</td>\n",
              "      <td>0</td>\n",
              "      <td>1992</td>\n",
              "      <td>0</td>\n",
              "      <td>98031</td>\n",
              "      <td>47.4034</td>\n",
              "      <td>-122.187</td>\n",
              "      <td>1720</td>\n",
              "      <td>3605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13488</th>\n",
              "      <td>4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1720</td>\n",
              "      <td>8638</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1720</td>\n",
              "      <td>0</td>\n",
              "      <td>1994</td>\n",
              "      <td>0</td>\n",
              "      <td>98003</td>\n",
              "      <td>47.2704</td>\n",
              "      <td>-122.313</td>\n",
              "      <td>1870</td>\n",
              "      <td>7455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20731</th>\n",
              "      <td>2</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1240</td>\n",
              "      <td>705</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1150</td>\n",
              "      <td>90</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98027</td>\n",
              "      <td>47.5321</td>\n",
              "      <td>-122.073</td>\n",
              "      <td>1240</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2358</th>\n",
              "      <td>3</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1280</td>\n",
              "      <td>13356</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1280</td>\n",
              "      <td>0</td>\n",
              "      <td>1994</td>\n",
              "      <td>0</td>\n",
              "      <td>98042</td>\n",
              "      <td>47.3715</td>\n",
              "      <td>-122.074</td>\n",
              "      <td>1590</td>\n",
              "      <td>8071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11964</th>\n",
              "      <td>3</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1000</td>\n",
              "      <td>6914</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "      <td>1947</td>\n",
              "      <td>0</td>\n",
              "      <td>98125</td>\n",
              "      <td>47.7144</td>\n",
              "      <td>-122.319</td>\n",
              "      <td>1000</td>\n",
              "      <td>6947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21575</th>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3087</td>\n",
              "      <td>5002</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3087</td>\n",
              "      <td>0</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>98023</td>\n",
              "      <td>47.2974</td>\n",
              "      <td>-122.349</td>\n",
              "      <td>2927</td>\n",
              "      <td>5183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2120</td>\n",
              "      <td>4780</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2120</td>\n",
              "      <td>0</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>98053</td>\n",
              "      <td>47.6810</td>\n",
              "      <td>-122.032</td>\n",
              "      <td>1690</td>\n",
              "      <td>2650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>380</td>\n",
              "      <td>15000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>380</td>\n",
              "      <td>0</td>\n",
              "      <td>1963</td>\n",
              "      <td>0</td>\n",
              "      <td>98168</td>\n",
              "      <td>47.4810</td>\n",
              "      <td>-122.323</td>\n",
              "      <td>1170</td>\n",
              "      <td>15000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3130</td>\n",
              "      <td>5999</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3130</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>98042</td>\n",
              "      <td>47.3837</td>\n",
              "      <td>-122.099</td>\n",
              "      <td>3020</td>\n",
              "      <td>5997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16209 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4739de1-8ddb-492f-91bb-2caafdc16250')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4739de1-8ddb-492f-91bb-2caafdc16250 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4739de1-8ddb-492f-91bb-2caafdc16250');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       bedrooms  bathrooms  sqft_living  sqft_lot  floors waterfront  view  \\\n",
              "5945          4       2.25         1810      9240     2.0          0     0   \n",
              "8423          3       2.50         1600      2788     2.0          0     0   \n",
              "13488         4       2.50         1720      8638     2.0          0     0   \n",
              "20731         2       2.25         1240       705     2.0          0     0   \n",
              "2358          3       2.00         1280     13356     1.0          0     0   \n",
              "...         ...        ...          ...       ...     ...        ...   ...   \n",
              "11964         3       1.50         1000      6914     1.0          0     0   \n",
              "21575         3       2.50         3087      5002     2.0          0     0   \n",
              "5390          3       2.50         2120      4780     2.0          0     0   \n",
              "860           1       0.75          380     15000     1.0          0     0   \n",
              "15795         4       2.50         3130      5999     2.0          0     0   \n",
              "\n",
              "       condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
              "5945           3      7        1810              0      1961             0   \n",
              "8423           4      7        1600              0      1992             0   \n",
              "13488          3      8        1720              0      1994             0   \n",
              "20731          3      7        1150             90      2009             0   \n",
              "2358           3      7        1280              0      1994             0   \n",
              "...          ...    ...         ...            ...       ...           ...   \n",
              "11964          3      7        1000              0      1947             0   \n",
              "21575          3      8        3087              0      2014             0   \n",
              "5390           3      7        2120              0      2004             0   \n",
              "860            3      5         380              0      1963             0   \n",
              "15795          3      7        3130              0      2006             0   \n",
              "\n",
              "      zipcode      lat     long  sqft_living15  sqft_lot15  \n",
              "5945    98055  47.4362 -122.187           1660        9240  \n",
              "8423    98031  47.4034 -122.187           1720        3605  \n",
              "13488   98003  47.2704 -122.313           1870        7455  \n",
              "20731   98027  47.5321 -122.073           1240         750  \n",
              "2358    98042  47.3715 -122.074           1590        8071  \n",
              "...       ...      ...      ...            ...         ...  \n",
              "11964   98125  47.7144 -122.319           1000        6947  \n",
              "21575   98023  47.2974 -122.349           2927        5183  \n",
              "5390    98053  47.6810 -122.032           1690        2650  \n",
              "860     98168  47.4810 -122.323           1170       15000  \n",
              "15795   98042  47.3837 -122.099           3020        5997  \n",
              "\n",
              "[16209 rows x 18 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOr1Qoj9P4TT",
        "outputId": "ecee1766-853c-498f-d7db-55378da8ffa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16209, 18)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfBMdhqDqSSP"
      },
      "source": [
        "After preprocessing, specifically, OneHotEncoding, the dataset becomes sparse so it does not visualize nicely. However, we can still see that the shape is maintained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbmXU0_d45Gl",
        "outputId": "afbfe225-b7ca-4c61-c49b-0f2b0cf8ccc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((16209, 88), (16209,), (5404, 88), (5404,))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "f2XpLg1E7_1b",
        "outputId": "d5ad0deb-c400-45fb-bb09-e2f49ddcc5a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9d70a513-10ec-4acf-bc82-73f3acbd83d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(0, 0)\\t0.6774021493321546\\n  (0, 1)\\t0.1789...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t0.505...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(0, 0)\\t0.6774021493321546\\n  (0, 1)\\t0.5056...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(0, 0)\\t-1.465666346525148\\n  (0, 1)\\t0.1789...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t-0.14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16204</th>\n",
              "      <td>(0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t-0.80...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16205</th>\n",
              "      <td>(0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t0.505...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16206</th>\n",
              "      <td>(0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t0.505...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16207</th>\n",
              "      <td>(0, 0)\\t-2.5372005944537994\\n  (0, 1)\\t-1.78...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16208</th>\n",
              "      <td>(0, 0)\\t0.6774021493321546\\n  (0, 1)\\t0.5056...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16209 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d70a513-10ec-4acf-bc82-73f3acbd83d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d70a513-10ec-4acf-bc82-73f3acbd83d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d70a513-10ec-4acf-bc82-73f3acbd83d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                       0\n",
              "0        (0, 0)\\t0.6774021493321546\\n  (0, 1)\\t0.1789...\n",
              "1        (0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t0.505...\n",
              "2        (0, 0)\\t0.6774021493321546\\n  (0, 1)\\t0.5056...\n",
              "3        (0, 0)\\t-1.465666346525148\\n  (0, 1)\\t0.1789...\n",
              "4        (0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t-0.14...\n",
              "...                                                  ...\n",
              "16204    (0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t-0.80...\n",
              "16205    (0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t0.505...\n",
              "16206    (0, 0)\\t-0.3941320985964968\\n  (0, 1)\\t0.505...\n",
              "16207    (0, 0)\\t-2.5372005944537994\\n  (0, 1)\\t-1.78...\n",
              "16208    (0, 0)\\t0.6774021493321546\\n  (0, 1)\\t0.5056...\n",
              "\n",
              "[16209 rows x 1 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U-iL6kh5C4G",
        "outputId": "9e750bd0-8fe4-4186-ab11-1e06195526af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5945     268643.0\n",
              "8423     245000.0\n",
              "13488    200000.0\n",
              "20731    352499.0\n",
              "2358     232000.0\n",
              "           ...   \n",
              "11964    378000.0\n",
              "21575    399950.0\n",
              "5390     575000.0\n",
              "860      245000.0\n",
              "15795    315000.0\n",
              "Name: price, Length: 16209, dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpXO6iC59hXR"
      },
      "source": [
        "## Linear Regression + Elastic Net "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEWzVOGUOjfr"
      },
      "source": [
        "We will focus on the Linear Least Squares Regressor, without any cross validation performed nor with any feature selection as the baseline model.\n",
        "\n",
        "The Linear Regressor finds the 'hyperplane' of best fit for all observed feature points in the feature set. It uses the theory of the normal equations to derive feature coefficients $\\beta_j, 1\\le j \\le N$, where $N$ is the number of features.\n",
        "\n",
        "It also assumes that the residuals (i.e, errors) $\\epsilon_j$ are distributed normally-an important assumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jyjiOMBkkT",
        "outputId": "897f6568-bbda-4d29-8cae-7f0fd1c78e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train RMSE: 157885.61375305994, Test RMSE: 170289.19933769965\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "# instantiating linear regression model\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "# fitting the model on the training set,\n",
        "# and getting in sample predictions\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "lin_train_pred = lin_reg.predict(X_train_scaled)\n",
        "# calculating in-sample RMSE\n",
        "lin_train_rmse = np.sqrt(mean_squared_error(y_train, lin_train_pred))\n",
        "\n",
        "# getting predictions of the model on the test set,\n",
        "# and getting out-of-sample predictions\n",
        "lin_test_pred = lin_reg.predict(X_test_scaled)\n",
        "# calculating the out of sample RMSE\n",
        "lin_test_rmse = np.sqrt(mean_squared_error(y_test, lin_test_pred))\n",
        "\n",
        "# printing to console\n",
        "print(f'Train RMSE: {lin_train_rmse}, Test RMSE: {lin_test_rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tzUGBb7SFYK"
      },
      "source": [
        "Now that we have the baseline RMSE values to compare in sample and out of sampel performance with, we will try the [Elastic Net Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html).\n",
        "It is a linear regressor, but contains hyperparameters that control the bias/variance tradeoff, and the penalisation of the feature weights. \n",
        "\n",
        "We will use a [grid Search with 5 fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to help us tune the hyperparameters associated with tradeoff and penalisation, thereby solving the conundrum of overfitting as well.\n",
        "\n",
        "The L1 penalty, which is based on the L1 vector norm - the sum of absolute values of a vector, is of importance to us in this case. It automatically sets weak features' weights to zero-performing automatic feature selection. \n",
        "\n",
        "Noting that L1 regression (i.e., lasso regression) performs best when the number of observations far outnumber the number of features in the dataset, we make the hypothesis that the L1 norm will be the optimal penalisation hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fXA_7slz_1Ak"
      },
      "outputs": [],
      "source": [
        "# importing the ElasticNet Model\n",
        "from sklearn.linear_model import ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jSyssGKBZbwU"
      },
      "outputs": [],
      "source": [
        "# creating the grid for tuning -- the mixing percentage alpha, and the penalty parameter r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Z8gsY94Zvlz"
      },
      "outputs": [],
      "source": [
        "# creating the parameter grid\n",
        "param_grid={\n",
        "            # the bias-variance tradeoff parameter, to be in [0, 1]\n",
        "            'alpha': list(np.arange(11) / 10.0), \n",
        "            # the penalisation parameter, to be in (0.01, 1]\n",
        "            'l1_ratio': list(np.arange(2, 11) / 10.0)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b-ZvMuZebhrJ"
      },
      "outputs": [],
      "source": [
        "# creating the GridSearchCV object, \n",
        "# with 5 fold Cross validation on the Scaled Training Set\n",
        "best_elastic_net = GridSearchCV(estimator=ElasticNet(random_state=42),\n",
        "                                param_grid=param_grid,\n",
        "                                scoring='neg_root_mean_squared_error',\n",
        "                                n_jobs = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV264yOOdSji",
        "outputId": "e8f30b2e-a068-4c8d-ac0c-efeb98bf6d7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:622: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40550574079409.06, tolerance: 210410096861.09872\n",
            "  positive,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=ElasticNet(random_state=42), n_jobs=-1,\n",
              "             param_grid={'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,\n",
              "                                   0.9, 1.0],\n",
              "                         'l1_ratio': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,\n",
              "                                      1.0]},\n",
              "             scoring='neg_root_mean_squared_error')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fitting on the scaled training set\n",
        "best_elastic_net.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeeDzswiANRK",
        "outputId": "31319e86-c277-4fe7-bdcc-da66cac9ab01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alpha': 0.9, 'l1_ratio': 1.0}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_elastic_net.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeRNiJKrVg4x"
      },
      "source": [
        "We see that our hypothess holds true! the L1 penalty is the optimal penalisation parameter!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X90AWNfRc6Yv"
      },
      "outputs": [],
      "source": [
        "# storing the best model\n",
        "best_model = best_elastic_net.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hYRZu6iLdm5R"
      },
      "outputs": [],
      "source": [
        "# getting training and testing performance\n",
        "train_en_preds = best_model.predict(X_train_scaled)\n",
        "train_en_rmse = np.sqrt(mean_squared_error(y_true=y_train, y_pred=train_en_preds))\n",
        "\n",
        "test_en_preds = best_model.predict(X_test_scaled)\n",
        "test_en_rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=test_en_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69YEVKKnfYTf",
        "outputId": "1324c2c5-1268-4a31-ff1e-5f0b6c284e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train RMSE =  157885.73832942848\n",
            "Test RMSE =  170480.25155699125\n"
          ]
        }
      ],
      "source": [
        "# printing the train and test RMSE values\n",
        "print(\"Train RMSE = \", train_en_rmse)\n",
        "print(\"Test RMSE = \", test_en_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-lfzdrmkun5"
      },
      "source": [
        "Note that the in sample performance is similar, but the out of sample performance is slightly worse than simple linear regression. We will fix the L1 penalty and focus in tuning $\\alpha$ a bit more in a hope to remedy this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnR-cPAokpWA",
        "outputId": "a7ea66b8-31ff-4057-94aa-4ef78d6dd80c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 0.85, 'l1_ratio': 1.0}\n",
            "159140.56203369814\n",
            "Train RMSE =  157885.7248837366\n",
            "Test RMSE =  170480.39298843697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:622: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40547646310374.5, tolerance: 210410096861.09872\n",
            "  positive,\n"
          ]
        }
      ],
      "source": [
        "# creating the grid for tuning -- \n",
        "# the mixing percentage alpha, and the penalty parameter l1_ratio\n",
        "# \n",
        "param_grid={\n",
        "            #  setting an alpha neighbourhood \n",
        "            #  around the previous optimal value \n",
        "            #  to potentially arrive at the 'true optimum' \n",
        "            # percentage of bias/variance tradeoff\n",
        "            'alpha': list(np.arange(85, 101) / 100.0), \n",
        "            'l1_ratio': [1.0] # fixed L1 penalty.\n",
        "            }\n",
        "\n",
        "# creating the GridSearchCV object, \n",
        "# with 5 fold Cross validation on the Scaled Training Set\n",
        "best_elastic_net = GridSearchCV(estimator=ElasticNet(random_state=42),\n",
        "                                param_grid=param_grid,\n",
        "                                scoring='neg_root_mean_squared_error',\n",
        "                                n_jobs = -1, cv=5)\n",
        "\n",
        "# fitting on the scaled training set\n",
        "best_elastic_net.fit(X_train_scaled, y_train)\n",
        "\n",
        "# getting the optimal hyperparameters\n",
        "print(best_elastic_net.best_params_)\n",
        "\n",
        "# getting the minimised RMSE\n",
        "print(np.abs(best_elastic_net.best_score_))\n",
        "\n",
        "# storing the best model\n",
        "best_model = best_elastic_net.best_estimator_\n",
        "\n",
        "# getting training and testing performance\n",
        "train_en_preds = best_model.predict(X_train_scaled)\n",
        "train_en_rmse = np.sqrt(mean_squared_error(y_true=y_train, y_pred=train_en_preds))\n",
        "\n",
        "test_en_preds = best_model.predict(X_test_scaled)\n",
        "test_en_rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=test_en_preds))\n",
        "\n",
        "# printing the train and test RMSE values\n",
        "print(\"Train RMSE = \", train_en_rmse)\n",
        "print(\"Test RMSE = \", test_en_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm4em9DYnT6v"
      },
      "source": [
        "We again see that the Elastic net with the above hyperparameters performs poorly in comparision to the simple linear regressor, highlighting a need for further hyperparameter tuning or other methods to remove collinearity in the model.\n",
        "\n",
        "The true value of $\\alpha$ could lie in the interval from (0.8, 0.9), and could be the case of future tuning. measuring VIF or linear correlation between the features could be helpful cut down on some features before model fitting and hopefully result in better performance.\n",
        "\n",
        "PCA before running the regression could help the reduction of collinear features, but does run the risk of removing clearly highly correlated features with the label, leading to a worse performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDxeSyOi9wpP"
      },
      "source": [
        "## SVM Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx61KWW3rKhQ"
      },
      "source": [
        "In this section, I used Support Vector Machine Regression to predict the price of a home based off of our predictors. The support vector machine algorithm finds the Maximum-margin hyperplane and margins based off of different parameters such as our C value. As you will see below, I used Cross validation to find our optimal C value. Due to runtime constraints, I select a wide band of C values and used 3-5 test C values per CV run. I continually narrowed that range manuelly and stopped at our current state due to diminishing returns of further narrowing. For computational efficiency and added flexibility of the model, I used the kernel trick with a polynomial kernel. Instead of using polynomial predictor values, the polynomial kernel yields similar results with drastically better runtimes. Also, we can find non-linear regression boundries for our prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhrpvaRABkFD",
        "outputId": "f289392e-29ec-442a-d4b0-ece3d5f123bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=SVR(),\n",
              "             param_grid={'C': array([65000., 70000., 75000.]),\n",
              "                         'kernel': ['poly']})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVR # Hyperparameter CV for SVM \n",
        "C_param = np.linspace(start=65000, stop=75000, num=3)\n",
        "param = {'kernel' : ['poly'],'C' : C_param}\n",
        "\n",
        "modelsvr = SVR()\n",
        "\n",
        "grids = GridSearchCV(modelsvr,param,cv=5)\n",
        "\n",
        "grids.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1zRh0biFBkHC"
      },
      "outputs": [],
      "source": [
        "best_model_svr = grids.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrR8gfH6fXbx",
        "outputId": "ab18456f-10ac-4960-b588-15885344c269"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVR(C=75000.0, kernel='poly')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model_svr #Best Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9xj5J3mSBkJK"
      },
      "outputs": [],
      "source": [
        "# getting training and testing performance\n",
        "pred_train_svr = best_model_svr.predict(X_train_scaled)\n",
        "train_rmse_svr = np.sqrt(mean_squared_error(y_true=y_train, y_pred=pred_train_svr))\n",
        "\n",
        "pred_test_svr = best_model_svr.predict(X_test_scaled)\n",
        "test_rmse_svr = np.sqrt(mean_squared_error(y_true=y_test, y_pred=pred_test_svr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO3YqdtKBkLK",
        "outputId": "5645d225-fafb-4ebf-abc8-885d59f1e45a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "117901.4970217094"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_rmse_svr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrYaG2owelCE",
        "outputId": "6c0c0027-d10c-4b3d-cbfe-da104c18f4b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "142851.9597807096"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_rmse_svr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoLqJnIfswr2"
      },
      "source": [
        "Ultimately, the final model I ran used a C value of 75000 and had a training RMSE of 117901 and a test RMSE of 142851. Since the training RMSE and the test RMSE are not drastically different from one another we can see that our algorithm didnt overfit the training data with our chosen C value. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zndgPTm7-YlD"
      },
      "source": [
        "## AdaBoost Regression \n",
        "\n",
        "In this section we will try AdaBoost Regression to predict housing prices. In essence, AdaBoost is an ensemble algorithm that uses decision trees to train the model in succession, updating weights for each feature based on its performance in the previous iteration. So more weights will go towards features that were harder to determine. This calls for multiple hyperparameters, but we will look at three for this problem.\n",
        "\n",
        "Hyperparameter tuning was done for n_estimators and learning_rate for AdaBoostRegressor, and max_depth for DecisionTreeRegressor. To save time but still get the point across in the cell below, a wider range of hyperparameters were cross validated behind the scenes, then were narrowed to get these set of values to test over. Otherwise, it would take over 30 minutes to run the grid search. The first parameters I tested were the ones below, and were narrowed down based on their performance. \n",
        "```\n",
        "n_estimators_list = np.arange(5, 100, 5)\n",
        "learning_rate_list = [0.01, 0.1, 1.0, 5, 10]\n",
        "max_depth_list = np.arange(1, 10, 2)\n",
        "```\n",
        "The more max_depth there was in the decision tree, the lower the training error became. However, the testing error increased significantly, and Adaboost utilitzes small decision trees anyway, so I had to cut the depth to 5 as shown below. Even lower depths caused both the training and testing errors to increase significantly, which may indicate underfitting. \n",
        "\n",
        "The exact results may change slightly every iteration due to randomness. The errors here may be different from the ones on the slides. But the general pattern should be consistent. The errors on the slides are 138883 and 176715 for training and testing respectively. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cZjES7NSc2Qw"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor \n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore') # suppress unncessary warnings \n",
        "\n",
        "# hyperparameters: n_estimators, learning_rate\n",
        "n_estimators_list = [70, 75, 80]\n",
        "learning_rate_list = [0.09, 0.10, 0.11]\n",
        "max_depth_list = [4, 5]\n",
        "params = {'n_estimators': n_estimators_list, 'learning_rate': learning_rate_list}\n",
        "\n",
        "# Grid Search Cross Validation for different max_depth in DecisionTreeRegressor\n",
        "grids_per_tree = []\n",
        "for md in max_depth_list:\n",
        "    ada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=md))\n",
        "    grids_ada = GridSearchCV(ada, params, cv=5, n_jobs=-1)\n",
        "    grids_ada.fit(X_train_scaled, y_train)\n",
        "    grids_per_tree.append(grids_ada)\n",
        "\n",
        "# find best model out of all tree types \n",
        "best_model_ada = grids_per_tree[0].best_estimator_\n",
        "best_score = 0\n",
        "for grid in grids_per_tree:\n",
        "    if grid.best_score_ > best_score:\n",
        "        best_score = grid.best_score_\n",
        "        best_model_ada = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1RCyM-WZPJH",
        "outputId": "ccca0552-8a9a-42bf-fb79-9b21e3c45f45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=5),\n",
              "                  learning_rate=0.1, n_estimators=75)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model_ada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPoWhfCYc2Uz",
        "outputId": "c6871cb9-fb46-43c1-ada6-29b3d64c0ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADABOOST REGRESSOR: Train RMSE: 141129.49511566135, Test RMSE: 179574.28451463973\n"
          ]
        }
      ],
      "source": [
        "# train and test RMSE \n",
        "pred_train_ada = best_model_ada.predict(X_train_scaled.toarray())\n",
        "train_rmse_ada = np.sqrt(mean_squared_error(y_train, pred_train_ada))\n",
        "\n",
        "pred_test_ada = best_model_ada.predict(X_test_scaled.toarray())\n",
        "test_rmse_ada = np.sqrt(mean_squared_error(y_test, pred_test_ada))\n",
        "print(f'ADABOOST REGRESSOR: Train RMSE: {train_rmse_ada}, Test RMSE: {test_rmse_ada}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3bjIy8ijy32"
      },
      "source": [
        "Compared to the above two models, AdaBoost's performance is moderate. The training and testing errors are a little bit higher for AdaBoost which may indicate it is not the best model for housing prediction, although it is not the worst either. There is some difference between training and testing which may indicate slight overfitting so that is something to be cautious about. Changing the hyperparameters often caused extreme overfitting. However, it does not overfit as much as the Random Forest Regression below. If we had to choose one model from the four, this may not be the best one. It may be due to the sparseness of the data, since AdaBoost might not work well with very noisy data or data with many outliers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BShI2Ny-RGm"
      },
      "source": [
        "## Random Forest Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdQLT7JLg4KA"
      },
      "source": [
        "In this section we will look to see if the Random Forest Regression can successfuly predict housing prices in King County. Random Forest Regression is a Machine Learning technique that combines multiple ML algorithms to make more accurate predicitions. Essentially, this method constructs multiple decision trees during training and uses the average of these trees to predict results. Random Forest also takes into account feature selection, which determines the more important features of the model. We also wanted to see which features were the most important in the model, thus did so by using the feature_importances_ method.\n",
        "\n",
        "To determine the best model, we also decided to tune some of the hyperparameters. Specifically, n_estimators and ccp_alpha. n_estimators has a default of 100 and just determines the number of trees in the model. Therefore increasing the trees may increase the accuracy of the overall model. ccp_alpha is the way to regularize the minimal cost-complexity pruning. This parameter tries to ensure that overfitting does not occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wX6Ice2g_1E8"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "n_estimators_list = [50,100,200,500,750]\n",
        "alpha = [10**(-k) for k in range(0,10)]\n",
        "\n",
        "rf_rmse_train = []\n",
        "\n",
        "#decreasing size of X_train_scaled and y_train as process taking too much time\n",
        "X_mini = X_train_scaled[:1000,:]\n",
        "y_mini = y_train[:1000]\n",
        "\n",
        "for a in alpha:\n",
        "  for n_estimators in n_estimators_list:\n",
        "    model_rf = RandomForestRegressor(ccp_alpha= a, n_estimators=n_estimators,\n",
        "                                     n_jobs=-1)\n",
        "    model_rf.fit(X_mini, y_mini.ravel())\n",
        "    y_train_pred = model_rf.predict(X_mini)\n",
        "    rf_rmse_train.append(np.sqrt(mean_squared_error(y_mini, y_train_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reObn0zPFkOw",
        "outputId": "1538e18c-8d40-4dda-c683-6c560daf61ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4.12451804e-03, 2.13686316e-02, 3.08124574e-01, 1.60890517e-02,\n",
              "       2.49031871e-03, 2.50287368e-02, 3.62829361e-03, 9.18703193e-02,\n",
              "       2.21860865e-02, 2.80535440e-02, 3.19264107e-02, 3.45505847e-03,\n",
              "       1.98753231e-01, 5.78158646e-02, 8.56388017e-02, 1.92599882e-02,\n",
              "       1.16342557e-02, 1.24438804e-02, 1.98565098e-05, 4.56698201e-06,\n",
              "       1.22624716e-04, 2.21893286e-02, 6.34201341e-05, 7.51817073e-04,\n",
              "       3.05507867e-05, 5.06695518e-05, 1.74772944e-06, 9.39915344e-05,\n",
              "       1.38230710e-04, 3.76683080e-05, 4.83808708e-06, 1.94699202e-04,\n",
              "       6.37656430e-05, 1.05681325e-03, 9.78672946e-05, 8.18571132e-05,\n",
              "       1.99060839e-05, 1.73259826e-05, 8.41040278e-05, 1.08707050e-03,\n",
              "       9.53336629e-05, 5.22054002e-05, 9.90025633e-03, 1.55244735e-03,\n",
              "       8.50310674e-05, 4.44236686e-05, 2.39940841e-04, 1.19129735e-04,\n",
              "       2.31098010e-05, 9.64595997e-05, 7.30664461e-05, 1.18696072e-04,\n",
              "       9.91523441e-05, 1.75312773e-04, 2.91950061e-04, 6.22341522e-05,\n",
              "       1.78324285e-04, 4.02056176e-05, 1.67609313e-05, 1.04991459e-03,\n",
              "       3.21635724e-04, 6.50843457e-05, 4.35807875e-04, 8.36900719e-04,\n",
              "       4.57350165e-04, 5.93311008e-03, 3.93129787e-03, 1.24353536e-04,\n",
              "       3.10323593e-04, 2.27097013e-04, 5.40840176e-04, 3.86730571e-04,\n",
              "       4.22699169e-05, 4.31950069e-04, 1.06561286e-04, 1.73710358e-04,\n",
              "       9.30784057e-05, 1.93197777e-04, 4.93234116e-05, 1.30883095e-05,\n",
              "       9.54147540e-05, 1.46841788e-04, 4.17284351e-05, 2.33422939e-04,\n",
              "       1.12264754e-04, 2.16501478e-05, 1.66207623e-05, 2.40105879e-04])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "importances = model_rf.feature_importances_\n",
        "importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68a6F5Bvz2xc"
      },
      "source": [
        "As we can see the most important feature had the importance of 3.08E-1, this corresponds to the sqft living feature in our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtdDZCMbSmj9",
        "outputId": "8ea9151c-4536-4a38-f3f2-a628b56b77e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(alpha, n_estimators): [1, 50]\n",
            "55652.08855387276\n",
            "(alpha, n_estimators): [1, 100]\n",
            "57601.37684205916\n",
            "(alpha, n_estimators): [1, 200]\n",
            "57574.03785083581\n",
            "(alpha, n_estimators): [1, 500]\n",
            "58351.99491949761\n",
            "(alpha, n_estimators): [1, 750]\n",
            "56706.00872467165\n",
            "(alpha, n_estimators): [0.1, 50]\n",
            "57622.313314555744\n",
            "(alpha, n_estimators): [0.1, 100]\n",
            "56853.94479241195\n",
            "(alpha, n_estimators): [0.1, 200]\n",
            "57252.45816663593\n",
            "(alpha, n_estimators): [0.1, 500]\n",
            "56091.314196718995\n",
            "(alpha, n_estimators): [0.1, 750]\n",
            "57468.48753128135\n",
            "(alpha, n_estimators): [0.01, 50]\n",
            "60929.66313327818\n",
            "(alpha, n_estimators): [0.01, 100]\n",
            "60959.190430124814\n",
            "(alpha, n_estimators): [0.01, 200]\n",
            "55504.60029213453\n",
            "(alpha, n_estimators): [0.01, 500]\n",
            "57020.16333221189\n",
            "(alpha, n_estimators): [0.01, 750]\n",
            "57216.04184959641\n",
            "(alpha, n_estimators): [0.001, 50]\n",
            "61276.948603798766\n",
            "(alpha, n_estimators): [0.001, 100]\n",
            "57586.81314604916\n",
            "(alpha, n_estimators): [0.001, 200]\n",
            "54661.64683876278\n",
            "(alpha, n_estimators): [0.001, 500]\n",
            "57390.69976496571\n",
            "(alpha, n_estimators): [0.001, 750]\n",
            "57257.533467954236\n",
            "(alpha, n_estimators): [0.0001, 50]\n",
            "58515.51154005829\n",
            "(alpha, n_estimators): [0.0001, 100]\n",
            "56738.57181158085\n",
            "(alpha, n_estimators): [0.0001, 200]\n",
            "56209.18442528983\n",
            "(alpha, n_estimators): [0.0001, 500]\n",
            "55913.286537573906\n",
            "(alpha, n_estimators): [0.0001, 750]\n",
            "56783.671156952885\n",
            "(alpha, n_estimators): [1e-05, 50]\n",
            "55016.899794259225\n",
            "(alpha, n_estimators): [1e-05, 100]\n",
            "58322.22718918162\n",
            "(alpha, n_estimators): [1e-05, 200]\n",
            "57242.43238307046\n",
            "(alpha, n_estimators): [1e-05, 500]\n",
            "56986.97871497444\n",
            "(alpha, n_estimators): [1e-05, 750]\n",
            "57168.5707345642\n",
            "(alpha, n_estimators): [1e-06, 50]\n",
            "59613.786410394474\n",
            "(alpha, n_estimators): [1e-06, 100]\n",
            "57918.54677709211\n",
            "(alpha, n_estimators): [1e-06, 200]\n",
            "59437.53116987754\n",
            "(alpha, n_estimators): [1e-06, 500]\n",
            "57269.83371558336\n",
            "(alpha, n_estimators): [1e-06, 750]\n",
            "56643.212727972976\n",
            "(alpha, n_estimators): [1e-07, 50]\n",
            "60630.83556752665\n",
            "(alpha, n_estimators): [1e-07, 100]\n",
            "59731.94043445513\n",
            "(alpha, n_estimators): [1e-07, 200]\n",
            "56941.84915872375\n",
            "(alpha, n_estimators): [1e-07, 500]\n",
            "58753.6526900649\n",
            "(alpha, n_estimators): [1e-07, 750]\n",
            "56605.45025366844\n",
            "(alpha, n_estimators): [1e-08, 50]\n",
            "61687.48877792009\n",
            "(alpha, n_estimators): [1e-08, 100]\n",
            "58870.81284150004\n",
            "(alpha, n_estimators): [1e-08, 200]\n",
            "57334.38729634888\n",
            "(alpha, n_estimators): [1e-08, 500]\n",
            "55659.919673798046\n",
            "(alpha, n_estimators): [1e-08, 750]\n",
            "57236.68647003055\n",
            "(alpha, n_estimators): [1e-09, 50]\n",
            "59070.6635387415\n",
            "(alpha, n_estimators): [1e-09, 100]\n",
            "57905.74905421272\n",
            "(alpha, n_estimators): [1e-09, 200]\n",
            "56370.01798794705\n",
            "(alpha, n_estimators): [1e-09, 500]\n",
            "56921.325663597905\n",
            "(alpha, n_estimators): [1e-09, 750]\n",
            "57506.260122080115\n"
          ]
        }
      ],
      "source": [
        "o_alpha, o_n_estimator = alpha[0], n_estimators_list[0]\n",
        "min_rmse_train = rf_rmse_train[0]\n",
        "ind = 0\n",
        "\n",
        "for a in alpha:\n",
        "  for n_estimators in n_estimators_list:\n",
        "    if rf_rmse_train[ind] < min_rmse_train: \n",
        "      min_rmse_train = rf_rmse_train[ind]\n",
        "      o_alpha = a\n",
        "      o_n_estimator = n_estimators\n",
        "    print(\"(alpha, n_estimators):\", [a, n_estimators])\n",
        "    print(rf_rmse_train[ind])\n",
        "    ind += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZinaGz4T2tc",
        "outputId": "8b3df8e8-7cbe-43fc-edcb-8b2c7a2854dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.001 200\n"
          ]
        }
      ],
      "source": [
        "print(o_alpha, o_n_estimator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQUGFh1ZuaWx"
      },
      "source": [
        "We can double check our work by using the GridSearchCV function to determine the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "reGJCapvphqM"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "param_grid = {'n_estimators': n_estimators_list, 'ccp_alpha': alpha}\n",
        "\n",
        "X_mini = X_train_scaled[:1000,:]\n",
        "y_mini = y_train[:1000]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42) \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, n_jobs = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bswyiZbGqE5_",
        "outputId": "ba52a80e-0908-4f3e-855e-9752bcde1793"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.01, 'n_estimators': 750}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search.fit(X_mini, y_mini) \n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZKM7Q61CUGyP"
      },
      "outputs": [],
      "source": [
        "model_rf = RandomForestRegressor(ccp_alpha= o_alpha, n_estimators=o_n_estimator,\n",
        "                                n_jobs=-1)\n",
        "model_rf.fit(X_train_scaled, y_train.ravel())\n",
        "\n",
        "y_train_pred = model_rf.predict(X_train_scaled)\n",
        "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "\n",
        "y_test_pred = model_rf.predict(X_test_scaled)\n",
        "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-13jCdIC_ro7",
        "outputId": "363ecf96-a185-4878-93d3-5c3aaf7946df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train RMSE =  47119.36527978785\n",
            "Test RMSE =  142430.0964247123\n"
          ]
        }
      ],
      "source": [
        "print(\"Train RMSE = \", train_rmse_rf)\n",
        "print(\"Test RMSE = \", test_rmse_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAA4XYNz0KzR"
      },
      "source": [
        "Compared to the other models, although Random Forest Regression has the least error for both the training and testing sets, we do see a huge disparity between the values which does suggest that overfitting has occured. We did try to manually tune the parameters to see if that would fix this disparity however, we were unsuccessful. This suggests that some greater issue is occuring in our model which is something we would like to look more into in the future. However, Random Forest still has the lowest RMSE for the training set which does suggest it is the best model even though there is overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RSi-CX_ETk8"
      },
      "source": [
        "## CONCLUSIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "2WTAi-dl-Jiq",
        "outputId": "b3fd71f0-fc00-4d6c-891f-e9a3c03ab692"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdYAAAFHCAYAAAAcIG28AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7xVdZ3/8debgwKGCCopggoqXlARBc2mNO9hY14xRScY81KmY41d1N/0Gx3TGZ1Ja5wxzQsqZeKtEhMzM1Pnl5igeME0jogjBApyES8hBz6/P9Z3x+awzzkga++1D/v9fDz246z1WZf92Tvjs7/f9V3fpYjAzMzM8tGl6ATMzMw2JC6sZmZmOXJhNTMzy5ELq5mZWY5cWM3MzHLkwmpmZpajrkUnUC+23HLLGDhwYNFpmJl1KlOnTl0QEX2LzqOeuLAmAwcOZMqUKUWnYWbWqUh6vegc6o27gs3MzHLkwmpmZpYjF1YzM7McubCamZnlyIXVzMwsRy6sZmZmOXJhNTMzy5ELq5mZWY48QYSZdR6XbJbDOZas/znM2uEWq5mZWY6qVlgljZP0lqQXy2J3SpqWXrMkTUvxgZI+KNt2fdkxwyW9IKlZ0jWSlOKbS3pY0oz0t0+KK+3XLOl5SftU6zOamZm1Vs0W663AyPJARJwUEcMiYhhwL/Czss2vlrZFxFfK4tcBZwKD06t0zguBRyJiMPBIWgc4smzfs9LxZmZmNVG1whoRjwMLK21Lrc4vAHe0dw5J/YBeETE5IgIYDxybNh8D3JaWb2sVHx+ZyUDvdB4zM7OqK+oa6wHAmxExoyw2SNKzkh6TdECK9Qdml+0zO8UAtoqIuWl5HrBV2TFvtHHMaiSdJWmKpCnz589fj49jZmaWKaqwjmb11upcYLuI2Bs4H/ippF5re7LUmo11TSIiboiIERExom9fP07QzMzWX81vt5HUFTgeGF6KRcQyYFlanirpVWBnYA4woOzwASkG8KakfhExN3X1vpXic4Bt2zjGzMysqoposR4GvBwRf+3ildRXUlNa3oFs4NHM1NX7jqT903XZMcB96bCJwNi0PLZVfEwaHbw/sKSsy9jMzKyqqnm7zR3Ak8AukmZLOj1tOpk1By0dCDyfbr+5B/hKRJQGPn0VuAloBl4FHkzxK4DDJc0gK9ZXpPgkYGba/8Z0vJmZWU1UrSs4Ika3Ef/7CrF7yW6/qbT/FGCPCvG3gUMrxAM4Zx3TtXq0vrPseIYdMyuApzSsNy4mZmadmqc0NDMzy5ELq5mZWY5cWM3MzHLkwmpmZpYjF1YzM7McubCamZnlyIXVzMwsRy6sZmZmOXJhNTMzy5ELq5mZWY5cWM3MzHLkwmpmZpYjF1YzM7Mc+ek2ORp44QPrfY5Z3XNIpA74uzCzRuUWq5mZWY5cWM3MzHLkwmpmZpYjF1YzM7McubCamZnlqGqFVdI4SW9JerEsdomkOZKmpdfnyrZdJKlZ0iuSPlsWH5lizZIuLIsPkvRUit8paeMU75bWm9P2gdX6jGZmZq1Vs8V6KzCyQvz7ETEsvSYBSBoCnAzsno75oaQmSU3AtcCRwBBgdNoX4Mp0rp2ARcDpKX46sCjFv5/2MzMzq4mqFdaIeBxYuJa7HwNMiIhlEfEa0Azsl17NETEzIj4EJgDHSBJwCHBPOv424Niyc92Wlu8BDk37m5mZVV0R11jPlfR86iruk2L9gTfK9pmdYm3FtwAWR0RLq/hq50rbl6T91yDpLElTJE2ZP3/++n8yMzNreLUurNcBOwLDgLnAVTV+/9VExA0RMSIiRvTt27fIVMzMbANR08IaEW9GxIqIWAncSNbVCzAH2LZs1wEp1lb8baC3pK6t4qudK23fLO1vZmZWdTUtrJL6la0eB5RGDE8ETk4jegcBg4E/AE8Dg9MI4I3JBjhNjIgAHgVGpePHAveVnWtsWh4F/Dbtb2ZmVnVVm4Rf0h3AQcCWkmYDFwMHSRoGBDAL+DJAREyXdBfwEtACnBMRK9J5zgUeApqAcRExPb3FBcAESZcBzwI3p/jNwI8lNZMNnjq5Wp/RzMystaoV1ogYXSF8c4VYaf/LgcsrxCcBkyrEZ7KqK7k8/hfgxHVK1szMLCeeecnMzCxHLqxmZmY58oPOzerdJZvlcI4l638OM1srLqxmZp2Rf3DVLRdWM6uZgRc+sF7Hz+qeUyJmVeRrrGZmZjlyYTUzM8uRC6uZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHLmwmpmZ5ciF1czMLEcurGZmZjlyYTUzM8uRC6uZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHFWtsEoaJ+ktSS+Wxf5D0suSnpf0c0m9U3ygpA8kTUuv68uOGS7pBUnNkq6RpBTfXNLDkmakv31SXGm/5vQ++1TrM5qZmbVWzRbrrcDIVrGHgT0iYijwJ+Cism2vRsSw9PpKWfw64ExgcHqVznkh8EhEDAYeSesAR5bte1Y63szMrCaqVlgj4nFgYavYryOiJa1OBga0dw5J/YBeETE5IgIYDxybNh8D3JaWb2sVHx+ZyUDvdB4zM7OqK/Ia65eAB8vWB0l6VtJjkg5Isf7A7LJ9ZqcYwFYRMTctzwO2KjvmjTaOMTMzq6quRbyppH8CWoDbU2gusF1EvC1pOPALSbuv7fkiIiTFR8jjLLLuYrbbbrt1PdzMzGwNNW+xSvp74Cjg1NS9S0Qsi4i30/JU4FVgZ2AOq3cXD0gxgDdLXbzp71spPgfYto1jVhMRN0TEiIgY0bdv3xw+nZmZNbqaFlZJI4FvA0dHxPtl8b6SmtLyDmQDj2amrt53JO2fRgOPAe5Lh00Exqblsa3iY9Lo4P2BJWVdxmZmZlVVta5gSXcABwFbSpoNXEw2Crgb8HC6a2ZyGgF8IHCppOXASuArEVEa+PRVshHGPciuyZauy14B3CXpdOB14AspPgn4HNAMvA+cVq3PaGZm1lrVCmtEjK4QvrmNfe8F7m1j2xRgjwrxt4FDK8QDOGedkjUzM8uJZ14yMzPLkQurmZlZjlxYzczMclTIfaxmjWTghQ+s1/GzuueUiJnVhFusZmZmOXJhNTMzy5ELq5mZWY5cWM3MzHLkwmpmZpYjF1YzM7McubCamZnlyIXVzMwsRy6sZmZmOXJhNTMzy5ELq5mZWY5cWM3MzHLkwmpmZpYjF1YzM7McubCamZnlqN3CKumQsuVBrbYdX62kzMzMOquOWqzfK1u+t9W273R0cknjJL0l6cWy2OaSHpY0I/3tk+KSdI2kZknPS9qn7Jixaf8ZksaWxYdLeiEdc40ktfceZmZm1dZRYVUby5XWK7kVGNkqdiHwSEQMBh5J6wBHAoPT6yzgOsiKJHAx8AlgP+DiskJ5HXBm2XEjO3gPMzOzquqosEYby5XW1zw44nFgYavwMcBtafk24Niy+PjITAZ6S+oHfBZ4OCIWRsQi4GFgZNrWKyImR0QA41udq9J7mJmZVVXXDrbvIGkiWeu0tExaH9T2Ye3aKiLmpuV5wFZpuT/wRtl+s1OsvfjsCvH23sPMzKyqOiqsx5Qtf6/Vttbr6ywiQlKHLd9qvYeks8i6ndluu+2qmYaZmTWIdgtrRDxWvi5pI2APYE5EvPUR3/NNSf0iYm7qzi2dZw6wbdl+A1JsDnBQq/jvUnxAhf3be4/VRMQNwA0AI0aMqGqBNzOzxtDR7TbXS9o9LW8GPEd2LfNZSaM/4ntOBEoje8cC95XFx6TRwfsDS1J37kPAEZL6pEFLRwAPpW3vSNo/jQYe0+pcld7DzMysqjoavHRARExPy6cBf4qIPYHhwLc7OrmkO4AngV0kzZZ0OnAFcLikGcBhaR1gEjATaAZuBL4KEBELge8CT6fXpSlG2uemdMyrwIMp3tZ7mJmZVVVH11g/LFs+HLgbICLmpVtG2xURbbVqD62wbwDntHGeccC4CvEpZF3TreNvV3oPMzOzauuoxbpY0lGS9gY+BfwKQFJXoEe1kzMzM+tsOmqxfhm4Btga+HpEzEvxQ4EHqpmYmZlZZ9TRqOA/sebMSUTEQ2SDiszMzKxMu4VV0jXtbY+I8/JNx8zMrHPrqCv4K8CLwF3An1m7+YHNzMwaVkeFtR9wInAS0ALcCdwTEYurnZiZmVln1NE11reB64HrJQ0ATgZeknRBRPy4FgmamVn9mzp16se7du16E9ktkB3dcdKZrQRebGlpOWP48OEVZ/XrqMUKQHo26miye1kfBKbmlqKZmXV6Xbt2vWnrrbferW/fvou6dOmywU4Ru3LlSs2fP3/IvHnzbgKOrrRPR4OXLgX+FvgjMAG4KCJacs/UzMw6uz029KIK0KVLl+jbt++SefPmrTE5UUlHLdbvAK8Be6XXv6YZl0Q2WdLQvJI1M7NOrcuGXlRL0udss7u7o8L6UZ+5amZm1pA6Grz0eqW4pC5k11wrbjczs8Y28MIHhud5vllX/G27Y3vmzZvXdNBBB+0CsGDBgo26dOkSm2++eQvAtGnT/ti9e/c2W9OPP/74JuPGjdvi1ltvfSOPXDu6xtqLbGL8/mSPYnsYOBf4Btkj5G7PIwkzM7P1sfXWW694+eWXXwI4//zzt+nZs+eKSy+99M3S9uXLl7PRRhtVPPbAAw98/8ADD3w/r1w6GhL9Y2AX4AXgDOBRYBRwbEQck1cSZmZmeTvhhBMGnnLKKdsNHTp017PPPnvAo48+usmwYcN23W233Ybsvffeuz733HPdAH75y19uevDBB+8EWVE+8cQTB+633367DBgwYM/LLrvs4+v6vh1dY90hPX8VSTcBc4HtIuIv6/pGZmZmtTZ37tyNn3nmmZe7du3KwoULuzz99NMvb7TRRvziF7/Y9Nvf/vaAhx566NXWxzQ3N3f//e9//8rixYubdttttz2+9a1vze/WrdtaD8zqqLAuLy1ExApJs11Uzcysszj++OMXde2albqFCxc2nXTSSYNmzZrVXVIsX7684jS9RxxxxOIePXpEjx49WjbffPPls2fP7rrjjjsur7RvJR11Be8l6Z30WgoMLS1LemetP5mZmVkBevbsubK0fMEFF/T/zGc+s3TGjBnT77///uYPP/ywYg0sb502NTXR0tKyTvPkdzQquGldTmZmZlav3nnnnaYBAwZ8CPCjH/1oy2q9z1pNaWhmZrYuOro9pggXXHDBvDPOOGPQlVdeuc3hhx9etYfJuLCamdkG5eqrr/5zpfhhhx323qxZs14srV9zzTV/BjjqqKOWHnXUUUsrHTtjxozp6/r+NX8CgaRdJE0re70j6euSLpE0pyz+ubJjLpLULOkVSZ8ti49MsWZJF5bFB0l6KsXvlLRxrT+nmZk1ppoX1oh4JSKGRcQwYDjwPvDztPn7pW0RMQlA0hCyx9XtDowEfiipSVITcC1wJDAEGJ32BbgynWsnYBFweq0+n5mZNbain5l3KPBqW1MnJscAEyJiWUS8BjQD+6VXc0TMjIgPyZ6+c4yypwQcAtyTjr8NOLZqn8DMzKxM0YX1ZOCOsvVzJT0vaZykPinWHyifv3F2irUV3wJYXPZ4u1LczMys6gorrOm659HA3Sl0HbAjMIxshqerapDDWZKmSJoyf/78ar+dmZk1gCJbrEcCz0TEmwAR8WZErIiIlcCNZF29AHOAbcuOG5BibcXfBnpL6toqvoaIuCEiRkTEiL59++b0sczMrJEVebvNaMq6gSX1i4i5afU4oDQkeiLwU0lXA9sAg4E/kD1sfbCkQWSF82TglIgISaWHBUwAxgL31eDzmJlZySWb5frYOC5ZUrXHxkE2EX+3bt1WHn744e+tb6qFFFZJHwMOB75cFv53ScOAAGaVtkXEdEl3AS8BLcA5EbEinedc4CGgCRgXEaX7jS4AJki6DHgWuLnqH8rMzArT0WPjOvLb3/520549e67otIU1It4jG2RUHvtiO/tfDlxeIT4JmFQhPpNVXclmZtaAnnjiiU3OP//8bd9///0uffr0abn99ttnbb/99ssvu+yyj99yyy19m5qaYuedd/7LVVddNXv8+PF9u3TpEnfdddcWP/jBD/535MiR737U9/XMS2ZmtsGJCM4777ztHnjggeZtttmm5cYbb+zzzW9+s//dd98965prrtn69ddff6FHjx6xYMGCpi233HLFmDFj5q9rK7ctLqxmZrbBWbZsWZcZM2b0OOSQQ3YGWLlyJX379l0OsMsuu3xw3HHHDTr66KMXn3rqqbnPGezCamZmG5yIYKeddvpg2rRpL7fe9uijj8548MEHN73vvvs2+973vtfvlVdeWef5gNtT9AQRZmZmuevWrdvKhQsXdv3Nb37zMYBly5ZpypQp3VesWMGrr7668ec///ml11577Zx33323acmSJU2bbrrpiqVLl+byqFS3WM3MLH8d3B5TbV26dGHChAmvnnfeedstXbq0acWKFTr77LPf3HPPPZedcsopg5YuXdoUETrjjDPe2nLLLVeccMIJi0eNGrXjgw8+2NuDl8zMzMqUP/ptypQpr7TePnXq1DViQ4cOXfanP/3ppTze313BZmZmOXJhNTMzy5ELq5mZ5WHlypUrVXQStZA+58q2truwmplZHl6cP3/+Zht6cV25cqXmz5+/Gavms1+DBy+Zmdl6a2lpOWPevHk3zZs3bw827EbbSuDFlpaWM9rawYXVzMzW2/Dhw98ie8Z2w9uQf1WYmZnVnAurmZlZjlxYzczMcuTCamZmliMXVjMzsxy5sJqZmeXIhdXMzCxHLqxmZmY5KqywSpol6QVJ0yRNSbHNJT0saUb62yfFJekaSc2Snpe0T9l5xqb9Z0gaWxYfns7fnI7doKfZMjOz+lB0i/XgiBgWESPS+oXAIxExGHgkrQMcCQxOr7OA6yArxMDFwCeA/YCLS8U47XNm2XEjq/9xzMys0RVdWFs7BrgtLd8GHFsWHx+ZyUBvSf2AzwIPR8TCiFgEPAyMTNt6RcTkiAhgfNm5zMzMqqbIwhrAryVNlXRWim0VEXPT8jxgq7TcH3ij7NjZKdZefHaF+GoknSVpiqQp8+fPX9/PY2ZmVugk/J+OiDmSPg48LOnl8o0REZKimglExA3ADQAjRoyo6nuZmVljKKzFGhFz0t+3gJ+TXSN9M3Xjkv6+lXafA2xbdviAFGsvPqBC3MzMrKoKKaySPiZp09IycATZQ2MnAqWRvWOB+9LyRGBMGh28P7AkdRk/BBwhqU8atHQE8FDa9o6k/dNo4DFl5zIzM6uaorqCtwJ+nu6A6Qr8NCJ+Jelp4C5JpwOvA19I+08CPgc0A+8DpwFExEJJ3wWeTvtdGhEL0/JXgVuBHsCD6WVmVhcGXvjAeh0/q3tOiVjuCimsETET2KtC/G3g0ArxAM5p41zjgHEV4lOAPdY7WTMzs3VQb7fbmJmZdWourGZmZjlyYTUzM8uRC6uZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHLmwmpmZ5ciF1czMLEcurGZmZjlyYTUzM8uRC6uZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHLmwmpmZ5ciF1czMLEcurGZmZjlyYTUzM8uRC6uZmVmOal5YJW0r6VFJL0maLulrKX6JpDmSpqXX58qOuUhSs6RXJH22LD4yxZolXVgWHyTpqRS/U9LGtf2UZmbWqIposbYA34iIIcD+wDmShqRt34+IYek1CSBtOxnYHRgJ/FBSk6Qm4FrgSGAIMLrsPFemc+0ELAJOr9WHMzOzxlbzwhoRcyPimbS8FPgj0L+dQ44BJkTEsoh4DWgG9kuv5oiYGREfAhOAYyQJOAS4Jx1/G3BsdT6NmZnZ6gq9xippILA38FQKnSvpeUnjJPVJsf7AG2WHzU6xtuJbAIsjoqVVvNL7nyVpiqQp8+fPz+ETmZlZoyussErqCdwLfD0i3gGuA3YEhgFzgauqnUNE3BARIyJiRN++fav9dmZm1gC6FvGmkjYiK6q3R8TPACLizbLtNwK/TKtzgG3LDh+QYrQRfxvoLalrarWW729mZlZVRYwKFnAz8MeIuLos3q9st+OAF9PyROBkSd0kDQIGA38AngYGpxHAG5MNcJoYEQE8CoxKx48F7qvmZzIzMysposX6KeCLwAuSpqXY/yEb1TsMCGAW8GWAiJgu6S7gJbIRxedExAoASecCDwFNwLiImJ7OdwEwQdJlwLNkhdzMzKzqal5YI+J/AFXYNKmdYy4HLq8Qn1TpuIiYSTZq2MzMrKY885KZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHLmwmpmZ5ciF1czMLEcurGZmZjlyYTUzM8uRC6uZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHLmwmpmZ5ciF1czMLEcurGZmZjlyYTUzM8uRC6uZmVmOXFjNzMxy5MJqZmaWIxdWMzOzHG2whVXSSEmvSGqWdGHR+ZiZWWPYIAurpCbgWuBIYAgwWtKQYrMyM7NGsEEWVmA/oDkiZkbEh8AE4JiCczIzswagiCg6h9xJGgWMjIgz0voXgU9ExLmt9jsLOCut7gK8UtNEK9sSWFB0EnXC30XG38Mq/i5WqZfvYvuI6Ft0EvWka9EJFCkibgBuKDqPcpKmRMSIovOoB/4uMv4eVvF3sYq/i/q1oXYFzwG2LVsfkGJmZmZVtaEW1qeBwZIGSdoYOBmYWHBOZmbWADbIruCIaJF0LvAQ0ASMi4jpBae1tuqqa7pg/i4y/h5W8Xexir+LOrVBDl4yMzMryobaFWxmZlYIF1YzM7McubBa3ZA0aG1iZo1I0tfWJmbFc2EtmDLbdrxnQ7i3QuyemmdhdUVSt7WJNYCxFWJ/X+skrGMb5KjgziQiQtIkYM+icymKpF2B3YHNJB1ftqkX0L2YrIoh6fm2NpH95zK0lvnUiSeBfdYitkGSNBo4BRgkqfy2wV7AwmKysva4sNaHZyTtGxFPF51IQXYBjgJ6A58viy8Fziwko+KsBAL4KXA/8EGx6RRH0tZAf6CHpL3JflxAVlA2KSyx2vs9MJdsCsOryuJLgbZ+iFmBfLtNHZD0MrAT8DrwHg3aOpH0yYh4sug8ipZa8KPJfmS8RFZkfx0RLYUmVmOSxpJ1dY4gm/SlVFiXArdGxM8KSq0Qkj4GfBARKyXtDOwKPBgRywtOzVpxYa0DkravFI+I12udS5HSPxbXAVtFxB6ShgJHR8RlBadWGEknkT0C8cqI+I+i8ymCpBMiotL194YiaSpwANAH+H9kPzY+jIhTC03M1uDBS3UgIl5PRfQDsm7A0qvR3AhcBCwHiIjnyaajbCiS+kv6hqT/Af4O+EeyHxyNaoCkXmmg302SnpF0RNFJFUAR8T5wPPDDiDiRbGyC1RkX1jog6WhJM4DXgMeAWcCDhSZVjE0i4g+tYo3W/fkY2bXVjYDTyEaCPgBsLGnzInMr0Jci4h3gCGAL4IvAFcWmVAhJ+iRwKtl/E5BN2Wp1xoOX6sN3gf2B30TE3pIOJmupNJoFknYktdbTc3XnFptSzW1P9vm/zKpnBUO67g7sUERSBStdW/0cMD4ipktSewdsoL5O1qPz8/Qd7AA8WnBOVoGvsdaB0nMVJT0H7J0GJzwXEXsVnVstpX8obgD+BlhE1oL/u4iYVWReVixJt5CNDh4E7EXWSvtdRAwvNLGCSNokdQlbnXJhrQOSfgMcS9a9tQXwFrBvRPxNoYkVJI1+7BIRS4vOpdYkvQTcDtwRETOLzqceSOoCDANmRsRiSVsA/dM1+IaRuoFvBnpGxHaS9gK+HBFfLTg1a8WFtQ6UhtGTXfM+FdgMuD0i3i40sRqRNKa97RExvla5FC39Y3ky8AXgbeAO4M6I+HOhiRVM0tHAgWn1sYi4v8h8iiDpKWAUMDEi9k6xFyNij2Izs9Z8jbUORMR76ZabwRFxm6RNaKxBCfu2ET+arAuwYQprRDwHPAdcJGl/4CRgsqRXgZ9GxI2FJlgASVeQ/Tdyewqdl+55/j8FplWIiHij1eXlFUXlYm1zi7UOSDqTbKDK5hGxo6TBwPURcWjBqdVcGpRyKnAB2eQIlzdal19rkg4Cvg8MiYiGmyM3TfM4LCJWpvUm4NkGnEDlHuBq4L+BTwBfA0ZERMPdklbvfLtNfTgH+BTwDkBEzAA+XmhGNSapq6QzgD8ChwGjIuKkRi2qkvaVdLWk14FLgB8B2xSbVaF6ly1vVlgWxfoK2b8V/YE5ZNedzyk0I6vIXcH1YVlEfFjq4pHUlQaaIELSOWS/vh8BRjbyKGBJ/0p2fXURMAH4VETMLjarwv0b8KykR8luvTkQuLDYlGortdL/07MsdQ7uCq4Dkv4dWAyMAf4B+CrwUkT8U6GJ1YiklWQjoeez+g+KhpszWdI/A49GxBNpfQxwAtk80pdEREM+zURSP1Zdi/9DRMwrMp8ipJm4DomID4vOxdrnwloH0u0Ep5PNLCPgIeCmaJD/cdqaK7mkkeZMlvQMcFhELJR0IFmr9R/Iuv12i4hRhSZYEI8KBknjgd2AiWQP6wAgIq4uLCmryIXVrI5ImhYRw9LytcD8iLik9bZGUmFU8Gjg6UYbFSzp4krxiPiXWudi7XNhrQOSPkU2QGV7suvepS7QRpy+rqFJepFsBGxLepzgWRHxeGlbI96z6FHBq5PUEyAi3i06F6vMg5fqw81kTzCZiu9La3R3AI9JWkA2aUjpWutOwJIiEytYb6B0fbkhRwVL2gP4MbB5Wl8AjImI6YUmZmtwYa0PSyKiEZ9ms5ryBzmn9S5A90aaFzUiLpf0CNCP7OHmpS6lLmTXWhtRw48KTm4Azo+IR+Gv9zffSDa3ttURdwXXgXQNqQn4GbCsFI+IZwpLqgCSJpMN3Hk3rfckKy7+h6PBeVQwVHowRyM+rKMzcIu1Pnwi/R1RFgvgkAJyKVL38utGEfFumt7RGlS6p3tFRMyV9CzZ/1f6AQ1XWIGZkv4vWXcwZI+W9IMa6pALax2IiIOLzqFOvCdpn1JLXdJwsuuM1oDSVJ9XAu9K+i7wLeAZYG9J4yLiykITrL0vAf9C1rMVZNffv1RoRlaRu4ILJOnvIuInks6vtL3R7k+TtC/ZfZt/JruWtjVwUrowSqoAAAnxSURBVERMLTQxK4Sk6cCngU3JprrcPiIWpF6MpyNi90ITrBFJx0fEz9Jyn4hYVHRO1j63WIv1sfR30wrbGu4XT0Q8LWlXYJcUeiUilheZkxXqw1REFklqjogFABHxvqRGmn3oO2StVMim/dynwFxsLbiwFigifpT+rnGDt6Sv1z6jYkg6JCJ+K+n4Vpt2lkTp17o1nB6S9iYbEb1xWlZ6dS80s9pSG8tWp1xY69f5wA+KTqJGPgP8Fvh8hW3Bql/r1ljmkj0mDbLBSuWXRhpp8FL5D4zuZT8wgMa7e6Az8DXWOiXpjYjYtug8aknSoIh4raOYWSNJ9++2JSKi0e4eqHsurHVK0v9GxHZF51FLkp6JiH1axaZGxPCicrL6kGYdGkJZF3BEjC8uI7O2uSu4QJKWUnmQkoAeNU6nMGnA0u7AZq2us/aisa6lWQVp8vmDyArrJOBI4H8AF1arSy6sBYqISqOBG9EuwFFk88GWX2ddCpxZSEZWT0YBe5FNvH+apK2AnxSck1mbXFitcBFxH3CfpE9GxJNF52N154OIWCmpRVIv4C2gocYfWOfiwmr15Lg0KcAHwK+AocA/RoRbJ41tiqTeZBPOTwXeBRryB5ikocBAyv7t9u1o9ceDl6xulB7kLek4sq7h84HHPcm4lUgaCPSKiOcLTqXmJI0j+7E5HViZwhERntawzrjFavVko/T3b4G7I2KJ5PvhG5WkNmcYKp9TuoHsHxFDik7COubCavXkfkkvk3UFny2pL/CXgnOy4lyV/nYne/LTc2Qj5ocCU4BPFpRXUZ6UNCQiXio6EWufu4KtrkjanOzB7yvSZOu9GvHZm7aKpJ8BF0fEC2l9D+CSiBhVbGa1JekzwESyWaeWkf3IiIgYWmhitgYXVqsrngjAWpM0vfWTbCrFNnSSmsnGHbzAqmusRMTrhSVlFbkr2OqGJwKwNjwv6SZW3bt6Klm3cKOZHxETi07COuYWq9UNSS+waiKAvUoTAUTE4QWnZgWS1B04GziArPtzKjAoIk4vNLEak/RDsklU7ifrCgZ8u009covV6oknArA1RMRfJP0O2Ab4AllxubfQpIrRg6ygHlEW89Of6pALq9UTTwRgfyVpZ2B0ei0A7gSIiIOLzKsoEXFa0TnY2nFXsNWlRp4IwDKSVgJPAKdHRHOKzYyIHYrNrBiSBgD/BXwqhZ4AvhYRs4vLyirpUnQCZpL2af0CNge6tjdJgG3wjid72Pmjkm6UdChlD/huQLeQ3W6zTXrdn2JWZ9xitcL5Qc7WHkkfA44h6xI+hGyU+M8j4teFJlZjpSk/O4pZ8VxYzazTkNQHOBE4KSIOLTqfWpL0CFkL9Y4UGg2c1mjfQ2fgrmArnKRvly2f2Grbv9Y+I6tXEbEoIm5o0GLyJbJR0fPIushHAR7QVIfcYrXCSXomIvZpvVxp3cys3vl2G6sHamO50rpZQ5H0X2T3q1YUEefVMB1bC+4KtnoQbSxXWjdrNFPI7uvuDuwDzEivYcDGBeZlbXBXsBVO0grgPbLWaQ/g/dImoHtEbNTWsWaNQtJk4NMR0ZLWNwKeiIj9i83MWnNXsBUuIpqKzsGsE+gD9AIWpvWeKWZ1xoXVzKxzuAJ4Nt33LeBA4JJCM7KK3BVsZtZJSNoa+ERafSoi5hWZj1Xmwmpm1klI6g9sT1lvY0Q8XlxGVom7gs3MOgFJVwInAdOBlSkcgAtrnXGL1cysE5D0CjA0IpZ1uLMVyvexmpl1DjMB33rWCbgr2Mysc3gfmJYm4/9rq9UzL9UfF1Yzs85hYnpZnfM1VjMzsxy5xWpm1glIGgz8GzCEbN5gACJih8KSsoo8eMnMrHO4BbgOaAEOBsYDPyk0I6vIXcFmZp2ApKkRMVzSCxGxZ3ms6Nxsde4KNjPrHJZJ6gLMkHQuMIdsIn6rM26xmpl1ApL2Bf4I9Aa+C2wGXBkRTxWamK3BhdXMrBOS1AScHBG3F52Lrc6Dl8zM6pikXpIukvTfko5Q5lygGfhC0fnZmtxiNTOrY5LuAxYBTwKHAh8nex7r1yJiWpG5WWUurGZmdazVKOAmYC6wXUT8pdjMrC3uCjYzq2/LSwsRsQKY7aJa39xiNTOrY5JWAO+VVoEeZBPyC4iI6FVUblaZC6uZmVmO3BVsZmaWIxdWMzOzHLmwmpmZ5ciF1RqWpBWSpkl6UdL9knoXnVOJpEslHZbDeQ6SFJLOKIsNS7FvrsN5Bkp6cX33MWsELqzWyD6IiGERsQewEDhnfU8oKZcHW0TEP0fEb/I4F/Aiq8/QMxp4Lqdzm1krLqxmmSeB/gCSdpT0K0lTJT0hadey+GRJL0i6TNK7KX5Q2m8i8JKkJkn/IelpSc9L+nLar5+kx8tayQekfW9N6y9I+se0762SRqXlQyU9m7aPk9QtxWdJ+hdJz6Rtu7bx2V4HukvaSpKAkcCDpY2pBTs55fpzSX1SfLik5yQ9R9mPjrY+n5llXFit4aXZbA4FJqbQDcA/pOdcfhP4YYr/J/CfaRac2a1Osw/ZFHM7A6cDSyJiX2Bf4ExJg4BTgIciYhiwFzANGAb0j4g90nlvaZVbd+BW4KS0vStwdtkuCyJiH7IHYLfXtXsPcCLwN8AzwLKybeOBCyJiKPACcHGK35K+h71anautz2dmuLBaY+shaRowD9gKeFhST7Lic3fa9iOgX9r/k8Ddafmnrc71h4h4LS0fAYxJxz8FbAEMBp4GTpN0CbBnRCwFZgI7SPovSSOBd1qddxfgtYj4U1q/DTiwbPvP0t+pwMB2PutdZIV1NHBHKShpM6B3RDxWfv50vbl3RDye4j8uO1dbn8/McGG1xvZBaj1uTzaLzTlk/59YnK69ll67rcW53itbFllLr3T8oIj4dSpSB5I9oPpWSWMiYhFZ6/V3wFeAm9bxM5RanivIWrMVRcQ8sqnxDgceWcf3aK3i51vPc5ptMFxYreFFxPvAecA3yKaKe03SiQDpEV2lrtDJwAlp+eR2TvkQcLakjdI5dpb0MUnbA29GxI1kBXQfSVsCXSLiXuA7ZF3K5V4BBkraKa1/EXiMj+afybp8V5QCEbEEWCTpgPLzR8RiYLGkT6f4qR19vo+Yk9kGJ5cRjGadXUQ8K+l5sq7SU4HrJH0H2AiYQDaK9uvATyT9E/ArYEkbp7uJrFv2mTRYaD5wLHAQ8C1Jy4F3gTFkA6ZukVT6kXtRq7z+Iuk0sq7prmTdydd/xM/4+zY2jQWul7QJWdf0aSl+GjBOUgDlLdK2Pp+Z4bmCzdZaKjwfRERIOhkYHRHHFJ2XmdUXt1jN1t5w4L9TK20x8KWC8zGzOuQWq5mZWY48eMnMzCxHLqxmZmY5cmE1MzPLkQurmZlZjlxYzczMcuTCamZmlqP/D+eADhFy2uEQAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# SUMMARY \n",
        "reg_types = ['Linear', 'Elastic Net', 'SVM', 'AdaBoost', 'Random Forest']\n",
        "types = ['Train', 'Test']\n",
        "trains = [lin_train_rmse, train_en_rmse, train_rmse_svr, train_rmse_ada, train_rmse_rf]\n",
        "tests = [lin_test_rmse, test_en_rmse, test_rmse_svr, test_rmse_ada, test_rmse_rf]\n",
        "\n",
        "data = {types[0]: trains, types[1]: tests}\n",
        "df = pd.DataFrame(data, columns=types, index=reg_types)\n",
        "\n",
        "df.plot(kind='bar').legend(bbox_to_anchor=(1.2, 0.5))\n",
        "plt.xlabel('Regression Model')\n",
        "plt.ylabel('RMSE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQxBZbbG1Nms"
      },
      "source": [
        "In conclusion, we can see that the Random Forest Regression had the best performance amongst our models even though overfitting occured. SVM had the second lowest errors for both Training and Testing RMSE and does not show signs of overfitting. Although there is a slight disparity between the Training and Testing RMSE values for AdaBoost, we can see slight amounts of overfitting but it is definitely not as intense as the Random Forest Regression. We also see that Elastic Net performed the worst on the Training set with Linear Regression as a second. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk1pQ6gN2l30"
      },
      "source": [
        "# Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kGcVhWZ2v8l"
      },
      "source": [
        "- There could be scope for some extensive hyperparameter tuning to potentially reduce some out of sample RMSEs.\n",
        "\n",
        "- Physically examining collinearity and removing features could help feature selection (of numeric features), and thus result in a more efficient model.\n",
        "\n",
        "- Dimensionality reduction algorithms, especially PCA could help, as it works well on sparse matrices. However, the feature set’s correlation to the label set is not considered by the PCA algorithm, and might result in either a less overfit model, or a model that does worse than before.\n",
        "\n",
        "- Ensemble Methods such as voting regressors, or blenders are worth looking at as in most cases, they produce lower out of sample RMSE values.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CFRM421FinalProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
